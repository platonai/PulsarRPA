package ai.platon.pulsar.external.impl

import ai.platon.pulsar.common.config.ImmutableConfig
import ai.platon.pulsar.common.getLogger
import ai.platon.pulsar.dom.FeaturedDocument
import ai.platon.pulsar.external.*
import dev.langchain4j.data.message.SystemMessage
import dev.langchain4j.data.message.UserMessage
import dev.langchain4j.model.chat.ChatLanguageModel
import dev.langchain4j.model.output.FinishReason
import org.apache.commons.codec.digest.DigestUtils
import org.apache.commons.lang3.StringUtils
import org.ehcache.Cache
import org.ehcache.CacheManager
import org.ehcache.config.builders.CacheConfigurationBuilder
import org.ehcache.config.builders.CacheManagerBuilder
import org.ehcache.config.builders.ExpiryPolicyBuilder
import org.ehcache.config.builders.ResourcePoolsBuilder
import org.jsoup.nodes.Element
import java.io.IOException
import java.io.InterruptedIOException
import java.time.Duration

open class ChatModelImpl(
    private val langchainModel: ChatLanguageModel,
    private val conf: ImmutableConfig
) : ChatModel {
    private val logger = getLogger(this)

    private val cacheManager: CacheManager = CacheManagerBuilder.newCacheManagerBuilder()
        .withCache(
            "modelResponses",
            CacheConfigurationBuilder.newCacheConfigurationBuilder(
                String::class.java,
                ModelResponse::class.java,
                ResourcePoolsBuilder.heap(1000) // Maximum entries in cache
            ).withExpiry(ExpiryPolicyBuilder.timeToLiveExpiration(Duration.ofMinutes(10))) // TTL: 60 minutes
        )
        .build(true)

    private val responseCache: Cache<String, ModelResponse> =
        cacheManager.getCache("modelResponses", String::class.java, ModelResponse::class.java)

    override val settings = ChatModelSettings(conf)

    /**
     * Generates a response from the model based on a sequence of messages.
     * Typically, the sequence contains messages in the following order:
     * System (optional) - User - AI - User - AI - User ...
     *
     * @return The response generated by the model.
     */
    override fun call(userMessage: String) = call(userMessage, "")

    /**
     * Generates a response from the model based on a sequence of messages.
     * Typically, the sequence contains messages in the following order:
     * System (optional) - User - AI - User - AI - User ...
     *
     * @param userMessage The user message.
     * @param systemMessage The system message.
     * @return The response generated by the model.
     */
    override fun call(userMessage: String, systemMessage: String): ModelResponse {
        return callWithCache(userMessage, systemMessage)
    }

    /**
     * Generates a response from the model based on a sequence of messages.
     * Typically, the sequence contains messages in the following order:
     * System (optional) - User - AI - User - AI - User ...
     *
     * @param document An array of messages.
     * @return The response generated by the model.
     */
    override fun call(document: FeaturedDocument, prompt: String) = call(document.document, prompt)

    /**
     * Generates a response from the model based on a sequence of messages.
     * Typically, the sequence contains messages in the following order:
     * System (optional) - User - AI - User - AI - User ...
     *
     * @param ele The Element to ask.
     * @return The response generated by the model.
     */
    override fun call(ele: Element, prompt: String) = call(ele.text(), prompt)

    private fun callWithCache(userMessage: String, systemMessage: String): ModelResponse {
        if (userMessage.isBlank()) {
            logger.warn("No user message, return empty response")
            return ModelResponse("", ResponseState.OTHER)
        }

        val trimmedUserMessage = userMessage.take(settings.maximumLength).trim()
        // Generate a cache key based on the user and system messages
        val cacheKey = DigestUtils.md5Hex("$trimmedUserMessage|$systemMessage")


        // Check if the response is already cached
        val cachedResponse = responseCache.get(cacheKey)
        if (cachedResponse != null) {
            logger.debug("Returning cached response for key: $cacheKey")
            return cachedResponse
        }

        val um = UserMessage.userMessage(trimmedUserMessage)

        if (logger.isInfoEnabled) {
            val log = StringUtils.abbreviate(trimmedUserMessage, 100).replace("\n", " ")
            logger.info("▶ Chat - [len: {}] {}", trimmedUserMessage.length, log)
        }

        val response = try {
            if (systemMessage.isBlank()) {
                langchainModel.generate(um)
            } else {
                val sm = SystemMessage.systemMessage(systemMessage)
                langchainModel.generate(um, sm)
            }
        } catch (e: IOException) {
            logger.info("IOException | {}", e.message)
            return ModelResponse("", ResponseState.OTHER)
        } catch (e: RuntimeException) {
            if (e.cause is InterruptedIOException) {
                logger.info("InterruptedIOException | {}", e.message)
                return ModelResponse("", ResponseState.OTHER)
            } else {
                logger.warn("RuntimeException | {} | {}", langchainModel.javaClass.simpleName, e.message)
                throw e
            }
        } catch (e: Exception) {
            logger.warn("[Unexpected] Exception | {} | {}", langchainModel.javaClass.simpleName, e.message)
            return ModelResponse("", ResponseState.OTHER)
        }

        val u = response.tokenUsage()
        val tokenUsage = TokenUsage(u.inputTokenCount(), u.outputTokenCount(), u.totalTokenCount())
        val r = response.finishReason()
        val state = when (r) {
            FinishReason.STOP -> ResponseState.STOP
            FinishReason.LENGTH -> ResponseState.LENGTH
            FinishReason.TOOL_EXECUTION -> ResponseState.TOOL_EXECUTION
            FinishReason.CONTENT_FILTER -> ResponseState.CONTENT_FILTER
            else -> ResponseState.OTHER
        }

        val modelResponse = ModelResponse(response.content().text().trim(), state, tokenUsage)
        if (logger.isInfoEnabled) {
            val log = StringUtils.abbreviate(modelResponse.content, 100).replace("\n", " ")
            logger.info("◀ Chat - token: {} | [len: {}] {}", modelResponse.tokenUsage.totalTokenCount, modelResponse.content.length, log)
        }

        // Cache the response
        responseCache.put(cacheKey, modelResponse)
        logger.debug("Cached response for key: $cacheKey")

        return modelResponse
    }
}
