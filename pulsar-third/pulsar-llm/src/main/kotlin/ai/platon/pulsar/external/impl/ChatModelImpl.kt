package ai.platon.pulsar.external.impl

import ai.platon.pulsar.common.concurrent.ConcurrentExpiringLRUCache
import ai.platon.pulsar.common.config.ImmutableConfig
import ai.platon.pulsar.common.getLogger
import ai.platon.pulsar.dom.FeaturedDocument
import ai.platon.pulsar.external.*
import dev.langchain4j.data.message.AiMessage
import dev.langchain4j.data.message.SystemMessage
import dev.langchain4j.data.message.UserMessage
import dev.langchain4j.model.chat.ChatLanguageModel
import dev.langchain4j.model.output.FinishReason
import dev.langchain4j.model.output.Response
import org.apache.commons.codec.digest.DigestUtils
import org.ehcache.Cache
import org.ehcache.CacheManager
import org.ehcache.config.builders.CacheConfigurationBuilder
import org.ehcache.config.builders.CacheManagerBuilder
import org.ehcache.config.builders.ExpiryPolicyBuilder
import org.ehcache.config.builders.ResourcePoolsBuilder
import org.jsoup.nodes.Element
import java.text.MessageFormat
import java.time.Duration
import java.util.concurrent.atomic.AtomicInteger

open class ChatModelImpl(
    private val langchainModel: ChatLanguageModel,
    private val conf: ImmutableConfig
) : ChatModel {
    private val logger = getLogger(this)

    private val failureCounts = ConcurrentExpiringLRUCache<String, AtomicInteger>()

    private val cacheManager: CacheManager = CacheManagerBuilder.newCacheManagerBuilder()
        .withCache(
            "modelResponses",
            CacheConfigurationBuilder.newCacheConfigurationBuilder(
                String::class.java,
                ModelResponse::class.java,
                ResourcePoolsBuilder.heap(1000) // Maximum entries in cache
            ).withExpiry(ExpiryPolicyBuilder.timeToLiveExpiration(Duration.ofMinutes(10))) // TTL: 60 minutes
        )
        .build(true)

    private val responseCache: Cache<String, ModelResponse> =
        cacheManager.getCache("modelResponses", String::class.java, ModelResponse::class.java)

    override val settings = ChatModelSettings(conf)

    /**
     * Generates a response from the model based on a sequence of messages.
     * Typically, the sequence contains messages in the following order:
     * System (optional) - User - AI - User - AI - User ...
     *
     * @return The response generated by the model.
     */
    override fun call(userMessage: String) = call(userMessage, "")

    /**
     * Generates a response from the model based on a sequence of messages.
     * Typically, the sequence contains messages in the following order:
     * System (optional) - User - AI - User - AI - User ...
     *
     * @param userMessage The user message.
     * @param systemMessage The system message.
     * @return The response generated by the model.
     */
    override fun call(userMessage: String, systemMessage: String): ModelResponse {
        return callWithCache(userMessage, systemMessage)
    }

    /**
     * Generates a response from the model based on a sequence of messages.
     * Typically, the sequence contains messages in the following order:
     * System (optional) - User - AI - User - AI - User ...
     *
     * @param document An array of messages.
     * @return The response generated by the model.
     */
    override fun call(document: FeaturedDocument, prompt: String) = call(document.document, prompt)

    /**
     * Generates a response from the model based on a sequence of messages.
     * Typically, the sequence contains messages in the following order:
     * System (optional) - User - AI - User - AI - User ...
     *
     * @param ele The Element to ask.
     * @return The response generated by the model.
     */
    override fun call(ele: Element, prompt: String) = call(ele.text(), prompt)

    private fun callWithCache(userMessage: String, systemMessage: String): ModelResponse {
        if (userMessage.isBlank()) {
            logger.warn("No user message, return empty response")
            return ModelResponse("", ResponseState.OTHER)
        }

        val userMessage1 = userMessage.take(settings.maximumLength)
        // Generate a cache key based on the user and system messages
        val cacheKey = DigestUtils.md5Hex("$userMessage1|$systemMessage")


        // Check if the response is already cached
        val cachedResponse = responseCache.get(cacheKey)
        if (cachedResponse != null) {
            logger.debug("Returning cached response for key: $cacheKey")
            return cachedResponse
        }

        val um = UserMessage.userMessage(userMessage1)

        val response = generateWithRetryOrNull(systemMessage, um) ?: return ModelResponse("", ResponseState.OTHER)

        val u = response.tokenUsage()
        val tokenUsage = TokenUsage(u.inputTokenCount(), u.outputTokenCount(), u.totalTokenCount())
        val r = response.finishReason()
        val state = when (r) {
            FinishReason.STOP -> ResponseState.STOP
            FinishReason.LENGTH -> ResponseState.LENGTH
            FinishReason.TOOL_EXECUTION -> ResponseState.TOOL_EXECUTION
            FinishReason.CONTENT_FILTER -> ResponseState.CONTENT_FILTER
            else -> ResponseState.OTHER
        }

        val modelResponse = ModelResponse(response.content().text().trim(), state, tokenUsage)

        // Cache the response
        responseCache.put(cacheKey, modelResponse)
        logger.debug("Cached response for key: $cacheKey")

        return modelResponse
    }

    @Throws(RuntimeException::class)
    private fun generateWithRetryOrNull(systemMessage: String, um: UserMessage): Response<AiMessage>? {
        return try {
            generateWithRetry(systemMessage, um)
        } catch (e: RuntimeException) {
            logger.warn("Model call failed after retries | ${e.message}")
            null
        }
    }

    @Throws(RuntimeException::class)
    private fun generateWithRetry(systemMessage: String, um: UserMessage): Response<AiMessage>? {
        var i = 0
        val n = 3

        while (i++ <= n) {
            try {
                return generate0(systemMessage, um)
            } catch (e: RuntimeException) {
                val modelClass = langchainModel.javaClass.simpleName
                val messageDetails = MessageFormat.format("$i/$n | {0} | {1}", modelClass, e.message)
                if (i < 3) {
                    val message = "Model call failure, retrying..."
                    val totalLogCount = failureCounts.computeIfAbsent(message) { AtomicInteger() }.incrementAndGet()
                    if (totalLogCount < 20) {
                        logger.info("{} {} | enable debug to show every failure", message, messageDetails)
                    } else {
                        logger.debug("[Verbose] {} {}", message, messageDetails)
                    }
                } else {
                    throw RuntimeException("Model call failure, retry failed | $messageDetails")
                }
            }
        }

        throw RuntimeException("Model call failure")
    }

    @Throws(RuntimeException::class)
    private fun generate0(systemMessage: String, um: UserMessage): Response<AiMessage>? {
        return if (systemMessage.isBlank()) {
            langchainModel.generate(um)
        } else {
            val sm = SystemMessage.systemMessage(systemMessage)
            langchainModel.generate(um, sm)
        }
    }
}
