= Frequently Asked Questions
Vincent Zhang <ivincent.zhang@gmail.com>
3.0, July 29, 2022: Frequently Asked Questions
:toc:
:icons: font
:url-quickref: https://docs.asciidoctor.org/asciidoc/latest/syntax-quick-reference/

Content entered directly below the header but before the first section heading is called the preamble.

== Advanced topics
=== How to scrape a million product pages from an e-commerce website a day?

Extracting web data at scale is extremely hard. Websites change frequently and are becoming more complex, meaning web data collected is often inaccurate or incomplete.

The right data extraction method can preempt many data-centered issues such as:

* Data that’s out-of-date
* Data that’s inaccurate
* Data with no context
* Data without sources or lineage
* Data that’s unusable due to structure

The right data extraction method for you should also address practical concerns such as:

* Your level of technical know-how
* Your budget
* The schedule on which you need your data updated
* The quantity of data you need
* Whether you need homogeneous or heterogeneous structured data
* Whether you need data from a single site, a range of sites, or from across the whole web

Here is a complete solution to scrape amazon.com at scale: link:https://github.com/platonai/exotic[TODO]

=== How to scrape pages behind a login?

It's fair simple to sign in a website before scraping in Pulsar:

```kotlin
val options = session.options(args)
val loginHandler = TaobaoLoginHandler(username, password, warnUpUrl = portalUrl)
options.ensureEventHandler().loadEventHandler.onAfterBrowserLaunch.addLast(loginHandler)

session.loadOutPages(portalUrl, options)
```

The key point is: adding a LoginHandler to perform just after the browser launching. The login event handler will open the login page and automatically type the username, password and other information required to sign in the website.

The example code can be found here: link:../../pulsar-app/pulsar-examples/src/main/kotlin/ai/platon/pulsar/examples/sites/topEc/chinese/login/tmall/TmallCrawler.kt[TmallCrawler].

=== How to download resources directly within a browser context?

There are many cases we want to download resources directly without a browser rendering:

. The data is returned by an AJAX request and easy to parse
. Non-browser-rendering scraping is super-fast

But the resources are often protected within a browsing session, we can not just issue http requests directly to ask the resources. To simulate the browsing session, we have to ask the resources inside a browsing context:

TODO:

=== How to scrape a single page application (SPA)?

There are several ways to scrape data from an SPA:

==== Resource mode

```kotlin
session.loadResource(url)
```

==== RPA mode
```kotlin

class RPAPaginateHandler(val initPageNumber: Int) : AbstractWebPageWebDriverHandler() {
    override suspend fun invokeDeferred(page: WebPage, driver: WebDriver): Any? {
        // ...
        // extract the text
        val text = driver.firstText(selector)
        // click the next page link
        driver.click(nextPageSelector)
        return null
    }
}
```

=== How to make sure all fields are extracted correctly?

. Wait for the page loaded completely
. Scroll down to the bottom of the page to ensure all ajax requests are triggered and loaded
. Simulate how a human being visits the webpage
. If there is still missing fields, consider refresh the page

=== How to crawl paginated links?

. Construct the urls
. Extract the pagination urls

=== How to crawl newly discovered links?

User a ListenableHyperlink to extract links after a referer page being fetched

=== How to crawl the entire website?



=== How to simulate human behaviors?

User event handler and web driver interface to interact with the browser.

=== How to schedule priority tasks?

. session.submit()
. globalCache.urlPool

=== How to start a task at a fixed time point?
=== How to drop a scheduled task?

. Use the load option *-deadTime*.

=== How to know the status of a task?
=== How to know what is happening in the system?

. Check the metrics
. Check the logs

=== How to automatically generate the css selectors for fields to scrape?

. Use link:https://github.com/platonai/exotic[Exotic]

=== How to extract content from websites using machine learning automatically with commercial accuracy?

. Use link:https://github.com/platonai/exotic[Exotic]

=== How to scrape amazon.com to match industrial needs?
We will release a complete solution to crawl the entire amazon website.
